# Crawler

Simple web crawler that limits its operation to a single domain.
For every visited page prints all its links and images.

### How to run
* Install Python 3.6 or newer. https://www.python.org/
* Install BeautifulSoup using pip package manager:
```bash
pip install beautifulsoup4
```
* Run Crawler with an URL given as command-line argument. Example:
```bash
python crawler.py http://example.com
```
### How it works
When a page is fetched, all links within current domain are added to a FIFO queue.
Crawler fetches all pages from the queue until the queue is empty.

### Limitations
* Only static HTML analysis is performed. Links generated by JavaScript are not detected.
* Subdomains are not penetrated.
* Not respecting robots.txt
* Possible infinite loop when pages have changing URLs.
* Single thread. It can stuck on some massive page.
* Identify pages by their full URL with protocol name. For example http://example.com and https://example.com
  are treated as different pages.
